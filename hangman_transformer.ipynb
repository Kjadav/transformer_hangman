{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import requests\n",
    "import random\n",
    "import string\n",
    "import secrets\n",
    "import time\n",
    "import re\n",
    "import collections\n",
    "import logging\n",
    "\n",
    "try:\n",
    "    from urllib.parse import parse_qs, urlencode, urlparse\n",
    "except ImportError:\n",
    "    from urlparse import parse_qs, urlparse\n",
    "    from urllib import urlencode\n",
    "\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "\n",
    "requests.packages.urllib3.disable_warnings(InsecureRequestWarning)\n",
    "\n",
    "import requests\n",
    "import time\n",
    "import json\n",
    "import re\n",
    "import collections\n",
    "import torch\n",
    "import numpy as np\n",
    "import random\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as torch_func\n",
    "from train_transformer import HangmanTransformer, Vocab\n",
    "logger = logging.getLogger(__name__)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def guess_model_transformer(model, vocab, masked_input, guessed_letters):\n",
    "    model.eval()\n",
    "    encoded = vocab.encode(masked_input)\n",
    "    input_tensor = torch.tensor(encoded, dtype=torch.long).unsqueeze(0)\n",
    "    attention_mask = (input_tensor != 0)\n",
    "\n",
    "    #to track the weighting consistently, and not backtrack on the weight, and not using gradients\n",
    "    with torch.no_grad():\n",
    "        output = model(input_tensor, mask=torch.bitwise_not(attention_mask))[0]\n",
    "        probs = torch_func.softmax(output, dim=-1)\n",
    "\n",
    "    masked_positions = []\n",
    "    for i, val in enumerate(encoded):\n",
    "        if val == 0:\n",
    "            masked_positions.append(i)\n",
    "\n",
    "    #creates evenly numbers in the range of 1.5-2, being a parameter to focus on the suffix methodology\n",
    "    # then we assign 0s to the matrix, and after, we are creating the probability for each position in the index. \n",
    "    position_weights = np.linspace(1.5, 2, len(masked_positions))\n",
    "    letter_scores = torch.zeros(probs.shape[-1])\n",
    "\n",
    "    for weight, pos in zip(position_weights, masked_positions):\n",
    "        letter_scores += weight * probs[pos]\n",
    "\n",
    "    #Build the list of (index, score) pairs and sorting the letter_scores\n",
    "    letter_scores = letter_scores.numpy()\n",
    "\n",
    "    indexed_scores = []\n",
    "    for i in range(len(letter_scores)):\n",
    "        #logger.info(f\"i in letters : {i}\")\n",
    "        indexed_scores.append((i, letter_scores[i]))\n",
    "    sorted_indexed_scores = sorted(indexed_scores, key=lambda x: -x[1])\n",
    "\n",
    "    sorted_indices = [idx for idx, _ in sorted_indexed_scores]\n",
    "    sorted_scores = [score for _, score in sorted_indexed_scores]\n",
    "    cumulative_probability = []\n",
    "    total = 0.0\n",
    "    for score in sorted_scores:\n",
    "        total += score\n",
    "        cumulative_probability.append(total)\n",
    "\n",
    "    probability_cutoff = 0.9 \n",
    "    cutoff = 0\n",
    "    for i, cp in enumerate(cumulative_probability):\n",
    "        if cp >= probability_cutoff:\n",
    "            cutoff = i\n",
    "            break\n",
    "\n",
    "    top_indices_select = sorted_indices[:cutoff + 1]\n",
    "\n",
    "    #removing guesseding letters\n",
    "    available_indices = [i for i in top_indices_select if vocab.index_character.get(i, '') not in guessed_letters]\n",
    "\n",
    "    if not available_indices:\n",
    "        for i in sorted_indices:\n",
    "            ch = vocab.index_character.get(i, '')\n",
    "            if ch not in guessed_letters:\n",
    "                return ch\n",
    "    \n",
    "    sampled_index = random.choice(available_indices)\n",
    "    return vocab.index_character.get(sampled_index, '_')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HangmanAPI(object):\n",
    "    def __init__(self, access_token=None, session=None, timeout=None):\n",
    "        self.access_token = access_token\n",
    "        self.session = session or requests.Session()\n",
    "        self.timeout = timeout\n",
    "        self.guessed_letters = []\n",
    "        \n",
    "        full_dictionary_location = \"xxx.txt\"\n",
    "        self.full_dictionary = self.build_dictionary(full_dictionary_location)        \n",
    "        self.full_dictionary_common_letter_sorted = collections.Counter(\"\".join(self.full_dictionary)).most_common()\n",
    "        \n",
    "        self.current_dictionary = []\n",
    "\n",
    "        self.model, self.vocab = self.load_transformer()\n",
    "\n",
    "    def load_transformer(self):\n",
    "        with open(\"xxx.txt\") as f:\n",
    "            words = [line.strip().lower() for line in f]\n",
    "        vocab = Vocab(words)\n",
    "        vocab_size = max(vocab.character_index.values()) + 1\n",
    "        model = HangmanTransformer(vocab_size)\n",
    "        model.load_state_dict(torch.load(\"hangman_transformer.pt\", map_location=torch.device('cpu')))\n",
    "        model.eval()\n",
    "        return model, vocab\n",
    "    def guess(self, word): # word input example: \"_ p p _ e \"\n",
    "        word = word.lower().replace(\" \", \"\")\n",
    "        \n",
    "        #suffixes methodology\n",
    "        if word.endswith(\"___\") or word.endswith(\"__\") or word.endswith(\"_\"):\n",
    "            for suffixes_letter in ['s', 'e', 'd', 'y', 'r', 'n', 'g', 't', 'x']:\n",
    "                if suffixes_letter not in self.guessed_letters:\n",
    "                    return suffixes_letter\n",
    "                \n",
    "        guess_letter = guess_model_transformer(\n",
    "            model=self.model,\n",
    "            vocab=self.vocab,\n",
    "            masked_input=word,\n",
    "            guessed_letters=self.guessed_letters\n",
    "        )\n",
    "        return guess_letter\n",
    "    \n",
    "    #DEFINE YOUR FUNCTION TO PLAY THE GAME.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
